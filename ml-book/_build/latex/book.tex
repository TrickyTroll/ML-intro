%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,french]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Sonny]{fncychap}
\ChNameVar{\Large\normalfont\sffamily}
\ChTitleVar{\Large\normalfont\sffamily}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}




\title{Introduction à la reconnaissance optique de caractère}
\date{nov. 27, 2020}
\release{}
\author{Émile Bergeron, Samuel Paquin, Étienne Parent, Jérémie Sanfaçon}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{intro::doc}}


La documentation sur l’utilisation de ce site n’a pas encore été faite. :)


\chapter{Rapport Final}
\label{\detokenize{rapport_final:rapport-final}}\label{\detokenize{rapport_final::doc}}
Cette section contient notre rapport final.


\section{Introduction}
\label{\detokenize{intro_finale:introduction}}\label{\detokenize{intro_finale::doc}}

\subsection{Mise en contexte}
\label{\detokenize{intro_finale:mise-en-contexte}}
L’intelligence artificielle est au coeur de l’actualité depuis près d’une
décennie. Elle est déjà entrain de changer le monde , et ce, dans plusieurs
secteurs incluant la finance, la sécurité, la santé, la justice criminelle,
les moyens de transport, la publicité, et plusieurs autres.

Que ça soit des décisions sur l’investissement d’un portefeuille
d’un individu ou de la détection de fraude en identifiant des anormalités, l’intelligence
artificielle est de plus en plus présente dans le secteur de la finance. \sphinxcite{zbib:nytimes}

Du côté de la
sécurité, un excellent exemple serait \sphinxhref{https://en.wikipedia.org/wiki/Project\_Maven}{Project Maven}
un projet d” intelligence artificielle du \sphinxhref{https://en.wikipedia.org/wiki/The\_Pentagon}{Pentagon}
des États\sphinxhyphen{}Unis qui est capable de passer à travers plusieurs informations,
vidéos et photos pour détecter des dangers potentiels.

L’intelligence artificielle est très importante dans la santé avec des compagnies comme
\sphinxhref{https://www.merantix.com/}{Merantix}, une compagnie allemande qui a permis de détecter
des ganglions lymphatiques ainsi que des problèmes liés à ceux\sphinxhyphen{}ci tels que des lésions
ou des cancers. L’étude de séquence d’ADN par l’intelligence artificielle permet de détecter
des maladies génétiques et des cancers.

Un des domaines le plus importants en ce moment serait, les moyens de transport avec plus de \$80
milliards investis dans des véhicules de conduite autonome entre 2014 et 2017. L’intelligence
artificielle dans ce domaine aurait pour but de diminuer grandement l’erreur humaine dans les transports
et réduire à presque zéro les accidents si la majorité des autos était intelligente. De plus, cela réduirait
aussi grandement le trafic grâce à la communication entre les automobiles intelligentes. La compagnie \sphinxhref{https://www.tesla.com/}{Tesla}
en est déjà très avancée pour ce qui est de leur auto intelligente. \sphinxcite{zbib:gouvqc}

Comme on peut le voir, cette technologie a permis de multiples avancées dans des domaines où
il se fait extrêmement difficile de modéliser la problématique selon une
fonction mathématique particulière. L’analyse de langage en est un bon exemple.
Le travail ne peut être modélisé par une seule fonction mathématique puisque
les conditions souvent changeantes nécessiteraient une multitude de fonctions
différentes pour chaque environnement qui n’est pas réaliste. La solution est
plutôt « d’entraîner » un ordinateur à comprendre le monde qui l’entoure.
Pour continuer avec l’exemple de l’analyse du langage, une solution serait
de fournir à l’ordinateur une immense quantité d’exemples et de solutions afin
qu’il développe la capacité de prédire la solution à de nouveaux exemples.
\sphinxhref{https://github.com/openai/gpt-3}{GPT\sphinxhyphen{}3},
un nouveau modèle d’intelligence artificielle produit par
\sphinxhref{https://openai.com}{OpenAI}, a permis à des développeurs de créer un programme
lui\sphinxhyphen{}même capable de programmer à partir de demandes spécifiques faites par un
utilisateur.


\subsection{Le début de la découverte des inconvénients}
\label{\detokenize{intro_finale:le-debut-de-la-decouverte-des-inconvenients}}
Malgré les avancées incroyables que l’intelligence artificielle a déjà permis et
continuera de permettre dans le futur, elle n’est pas sans ses inconvénients.


\subsubsection{Le biais}
\label{\detokenize{intro_finale:le-biais}}
Au
courant des dernières années, les systèmes intelligents sont de plus en plus
reconnus coupables de discrimination envers certains groupes d’individus. Une
étude réalisée par le \sphinxhref{https://www.nist.gov/}{NIST} à étudié le taux d’erreur de
différents programmes de reconnaissance faciale en fonction des différences de
sexe et d’ethnicité des individus sur les photos analysées. L’étude
présente des taux d’erreur
jusqu’à cent fois plus élevés pour des personnes d’origine asiatique ou
africaine lorsque comparé à des personnes d’origine européenne \sphinxcite{zbib:nistbias}.
Le taux d’erreur est aussi plus élevé chez les femmes que chez les hommes, et
ce, peut importe l’origine.

Un autre résultat important de cette étude est que le taux d’erreur associé à la
reconnaissance de personnes asiatiques n’est pas présent dans des programmes
réalisés dans des pays d’Asie. Cette observation permet de déduire l’un des plus
grands problèmes liés à l’intelligence artificielle: le biais.

Contrairement à une fonction mathématique qui transforme un chiffre de manière
définie, les procédés menant à la reconnaissance faciale sont beaucoup plus
flous et souvent très mal compris. Plusieurs considèrent les programmes
entraînés comme des « boîtes noires ». Il est difficile de prédire ce qui sortira
de la boîte lorsque l’on y insère quelque chose, et il est encore plus difficile
de comprendre pourquoi le programme prend certaines décisions plus que d’autres.

Cette imprévisibilité inquiète plusieurs. Elle rend la tâche de corriger le
biais assez ardue. Elle fait aussi en sorte qu’il est difficile de prédire le
comportement du programme dans des cas extrêmes sans avoir à lui faire passer
des tests dans ces conditions. Le biais est donc un phénomène difficile à
corriger, ce qui entraîne des questionnements en rapport aux bienfaits de
l’utilisation de l’intelligence artificielle.

Certaines régions du monde
commencent à bannir l’utilisation de la reconnaissance faciale par les forces
de l’ordre. C’est le cas de la ville de Portland, en Oregon \sphinxcite{zbib:cnnportland}.
La ville a décidé de bannir l’utilisation de la technologie suite à des craintes
en liées à son manque de précision, surtout lorsqu’utilisée sur des individus
appartenant à une minorité visible.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{black_box}.png}
\caption{L’analogie de la boîte noire.}\label{\detokenize{intro_finale:boite-noire}}\end{figure}


\subsubsection{Une deuxième révolution industrielle}
\label{\detokenize{intro_finale:une-deuxieme-revolution-industrielle}}
Une autre inquiétude liée à l’intelligence artificielle est l’importante
quantité d’emplois qui risque de disparaître puisqu’ils seront maintenant
occupés par des ordinateurs. Ces inquiétudes sont justifiées. Plusieurs articles,
dont
\sphinxhref{https://www.cnbc.com/2019/01/14/the-oracle-of-ai-these-kinds-of-jobs-will-not-be-replaced-by-robots-.html}{celui\sphinxhyphen{}ci}
publié par CNN ainsi que
\sphinxhref{https://medium.com/@ChanPriya/15-jobs-that-will-never-be-replaced-by-ai-512bfbbed0d6}{cette publication}
sur Medium tentent de rassurer la population en mentionnant des emplois qui ne
pourraient apparemment jamais être remplacés par des ordinateurs. Ils mentionnent
entre autres les emplois créatifs, accompagnés des emplois nécessitant beaucoup
d’interactions humaines.

Pourtant, le domaine de l’IA avance chaque année, et il existe maintenant une
panoplie de programmes capable de
\sphinxhref{https://openai.com/blog/musenet/}{composer de la musique},
\sphinxhref{https://www.nvidia.com/en-us/research/ai-playground/}{maîtriser les arts visuels}
ainsi qu”\sphinxhref{https://www.youtube.com/watch?v=D5VN56jQMWM}{entretenir des conversations au téléphone}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{duplex}.jpeg}
\caption{Le PDG de Google présentant une démonstration de Google Duplex.}\label{\detokenize{intro_finale:duplex-presentation}}\end{figure}

Il est dangereux d’extrapoler le progrès qui a été fait au courant des dernières
années sur les décennies à venir. Certaines lois limitant le développement de
l’IA, ou des limitations physiques au présent rythme d’augmentation de la
puissance de calcul des ordinateurs pourraient survenir grandement ralentir
le développement de la technologie. Si nous tentons tout de même de le faire,
les inquiétudes vécues par plusieurs semblent raisonnables.


\subsection{Comprendre la technologie pour démystifier les inquiétudes}
\label{\detokenize{intro_finale:comprendre-la-technologie-pour-demystifier-les-inquietudes}}
Bien que les précédentes inquiétudes face à l’intelligence artificielle soient
totalement justifiées, elles ne sont pas sans solution. Si son développement
est fait de manière éthique et s’il est bien encadré, nous pourrions en retirer
plus d’avantages que d’inconvénient. Pour bien comprendre les inquiétudes, il
faut d’abord comprendre les enjeux. C’est pourquoi nous tenterons de répondre
à la question suivante.

\sphinxstyleemphasis{Quel est le fonctionnent de l’intelligence artificielle et comment devrait\sphinxhyphen{}elle
être utilisée afin de bénéficier l’être humain?}


\section{Les librairies nécessaires}
\label{\detokenize{explications_librairies:les-librairies-necessaires}}\label{\detokenize{explications_librairies::doc}}
Afin de réaliser ce projet dans des temps raisonnables, nous utilisons des
outils et des données réalisés par des organisations réputées comme Google,
\sphinxhref{https://numpy.org/}{Numpy} et la \sphinxhref{https://matplotlib.org/}{Matplotlib} development team.


\subsection{Tensorflow}
\label{\detokenize{explications_librairies:tensorflow}}
\DUrole{xref,myst}{Tensorflow} est une plateforme nous permettant d’accélérer
le développement de notre application d’apprentissage machine. La librairie
procure un \sphinxhref{https://en.wikipedia.org/wiki/API}{API} en Python donnant accès
à de multiples fonctions utilisées pour obtenir des données et les utiliser
pour entraîner notre programme.


\subsubsection{Historique}
\label{\detokenize{explications_librairies:historique}}
Développée par Google et rendue publique en 2015 \sphinxcite{zbib:wikitf}, la librairie
a depuis permis aux masses de développer toutes sortes d’applications
bénéficiant de l’intelligence artificielle. TensorFlow est une version polie
du système DistBelief. DistBelief est un produit du projet The Google Brain.
Après avoir été utilisé pendant quelques années pour des produits Google ainsi
que pour de la recherche, DistBelief est amélioré et rendu publique sous le
nom TensorFlow \sphinxcite{zbib:tfpaper}. Aujourd’hui, la
\sphinxhref{https://github.com/tensorflow/tensorflow}{page Github} de TensorFlow mentionne
plus de 100 000 utilisateurs et 2 780 contributeurs. La librairie est utilisée
par de multiples entreprises dont Google, Coca\sphinxhyphen{}Cola, airbnb, Twitter et Intel
\sphinxcite{zbib:tfmain}.


\subsubsection{Notre utilisation}
\label{\detokenize{explications_librairies:notre-utilisation}}
Nous utilisons TensorFlow afin de faciliter l’accès à nos données et afin
de créer notre modèle.


\paragraph{Accès aux données}
\label{\detokenize{explications_librairies:acces-aux-donnees}}
Pour entrainer notre modèle, nous utilisons la base de données
\sphinxhref{http://yann.lecun.com/exdb/mnist/}{MNIST}. Bien qu’elle soit très complète,
le format de cette base de donnée est assez complexe
({\hyperref[\detokenize{preprocessing::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Voir la section sur le \sphinxstyleemphasis{preprocessing})}}}}. Heureusement,
la libraire TensorFlow procure un
\sphinxhref{https://www.tensorflow.org/api\_docs/python/tf/keras/datasets/mnist/load\_data}{interface simple}
avec le langage de programmation que nous utilisons.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{p}{(}\PYG{n}{train\PYGZus{}data}\PYG{p}{,} \PYG{n}{train\PYGZus{}labels}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{n}{test\PYGZus{}data}\PYG{p}{,} \PYG{n}{test\PYGZus{}labels}\PYG{p}{)}\PYG{p}{)} \PYG{o}{=} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{load\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{num\PYGZus{}data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{train\PYGZus{}data}\PYG{p}{,} \PYG{n}{test\PYGZus{}data}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{num\PYGZus{}labels} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{hstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{train\PYGZus{}labels}\PYG{p}{,} \PYG{n}{test\PYGZus{}labels}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

La classe \sphinxcode{\sphinxupquote{mnist}} fournie par la librairie \sphinxcode{\sphinxupquote{Tensorflow}}nous permet de charger en mémoire toutes les données nécessaires en seulement trois lignes.


\paragraph{Entraînement du modèle}
\label{\detokenize{explications_librairies:entrainement-du-modele}}
Le domaine de l’IA s’avère assez complexe. Programmer et entraîner un modèle nécessite des connaissances en mathématiques avancées \sphinxcite{zbib:goodfellow-et-al-2016}.

\sphinxcode{\sphinxupquote{Tensorflow}}vise à accélérer le développement de l’intelligence artificielle ainsi que de rendre ce développement accessible aux masses. Afin de simplifier la réalisation de notre programme et afin de le rendre plus efficace, nous comptons donc utiliser les méthodes d’entraînement fournies par la librairie. Pour aider le lecteur à comprendre le fonctionnement du programme (et donc pour éviter le phénomène de la {[}boîte noire{]}(lien vers la section de la boîte noire), chaque fonction utilisée sera décortiquée et expliquée en détails.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model} \PYG{o}{=} \PYG{n}{keras}\PYG{o}{.}\PYG{n}{Sequential}\PYG{p}{(}\PYG{p}{[}
    \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Flatten}\PYG{p}{(}\PYG{n}{input\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{28}\PYG{p}{,} \PYG{l+m+mi}{28}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{p}{]}\PYG{p}{)}

\PYG{n}{model}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{optimizer}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{adam}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
	\PYG{n}{loss}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{losses}\PYG{o}{.}\PYG{n}{SparseCategoricalCrossentropy}\PYG{p}{(}\PYG{n}{from\PYGZus{}logits}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{,}
              \PYG{n}{metrics}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{accuracy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{train\PYGZus{}images}\PYG{p}{,} \PYG{n}{train\PYGZus{}labels}\PYG{p}{,} \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\end{sphinxVerbatim}

Seulement quelques lignes sont nécessaires à l’entraînement de notre modèle.


\subsection{Numpy}
\label{\detokenize{explications_librairies:numpy}}
\sphinxhref{https://numpy.org}{NumPy} est une librairie facilitant le calcul avec le langage de programmation que nous utilisons pour créer notre modèle.


\subsubsection{Historique}
\label{\detokenize{explications_librairies:id5}}
La libraire à été créée en 2005 et était à l’époque basée sur les librairies \sphinxhref{https://docs.python.org/3/library/numeric.html}{numériques et les modules mathématiques} de Python \sphinxcite{zbib:numpy}.
Numpy vise à rendre la réalisation de grands calculs numériques plus simples et optimisée. Plusieurs de ses capacités ont des fonctions homologue dans les logiciels \sphinxhref{https://www.mathworks.com}{Matlab} et \sphinxhref{https://maplesoft.com}{Maple}.


\subsubsection{Notre utilisation}
\label{\detokenize{explications_librairies:id7}}
Comme sera possible de l’observer tout au long de ce rapport, les mathématiques constituent le pilier principal sur lequel repose le domaine de l’intelligence artificielle. De plus, la plupart des composantes des réseaux neuronaux peuvent être représentés comme des matrices. Numpy permet d’utiliser certaines propriétés des matrices afin de paralléliser les calculs menant à l’entraînement du modèle.  De plus,  comme nous le verrons dans la section suivante, Numpy peut être utilisé afin de représenter des images comme des matrices, et donc faciliter les opérations sur chacun des pixels.


\subsection{Matplolib}
\label{\detokenize{explications_librairies:matplolib}}
\sphinxhref{https://matplotlib.org}{Matplotlib} est une librairie de visualisation implémentée dans le langage de programmation Python.


\subsubsection{Historique}
\label{\detokenize{explications_librairies:id8}}
La libraire est un projet à code source ouvert financé par \sphinxhref{https://numfocus.org}{Numfocus}. Déployé depuis plus de 17 ans cite\sphinxcode{\sphinxupquote{wikimatplotlib}},  la librairie permet, tout comme \sphinxcode{\sphinxupquote{Numpy}}, au langage de programmation Python de se rapprocher de l’application Matlab, tout en restant libre de droit.


\subsubsection{Notre utilisation}
\label{\detokenize{explications_librairies:id9}}
La librairie fourni un \sphinxstyleemphasis{API} simple à utiliser permettant de réaliser des graphiques directement dans nos notebooks. Nous utilisons aussi la fonctionnalité permettant de représenter des images comme suit:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{train\PYGZus{}data}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

Ce qui permet d’obtenir la figure suivante:

\sphinxincludegraphics{{plot-demo}.png}

\sphinxcode{\sphinxupquote{train\_data{[}0{]}}} est un \sphinxcode{\sphinxupquote{array}} de pixels, représentés par leur \sphinxcode{\sphinxupquote{grayscale value}}. Nous discuterons plus de ces termes dans la section suivante.


\section{Traitement antérieur à l’entrainement}
\label{\detokenize{preprocessing:traitement-anterieur-a-lentrainement}}\label{\detokenize{preprocessing::doc}}
Dans cette section, nous discuterons du traitement nécessaire afin d’utiliser des images pour entrainer un réseau neuronal. Nous discuterons aussi de l’importance de ce traitement, ainsi que de la raison pour laquelle il doit aussi être réalisé sur les images que nous voudrons par la suite reconnaître.


\subsection{Pourquoi faire du \sphinxstyleemphasis{preprocessing}?}
\label{\detokenize{preprocessing:pourquoi-faire-du-preprocessing}}
Comme nous en avons discuté dans la section précédente, notre programme utilise des méthodes fournies par la librairie \sphinxcode{\sphinxupquote{Tensorflow}} afin de charger les données dans le bon format. Malheureusement, dans une majorité des cas, les données ne vous seront pas fournies sur un plateau d’argent. Les programmes d’apprentissage machine visent à faire du calcul statistique sur le jeu de données fourni

\begin{sphinxadmonition}{note}{Note:}
Dans le domaine de l’IA, un jeu de données représente l’ensemble des données traitées ainsi que leur étiquettes.
\end{sphinxadmonition}


\subsection{Mise en bouche sur l’apprentissage machine}
\label{\detokenize{preprocessing:mise-en-bouche-sur-lapprentissage-machine}}
L’entrainement d’un modèle se rapproche beaucoup des mathématiques, plus précisément de la statistique, comme en témoigne le \sphinxstyleemphasis{Deep Learning Book} \sphinxcite{zbib:goodfellow-et-al-2016}. L’apprentissage machine vise à ingérer des quantités massives de données provenant de sources différentes. Par la suite, à l’aide de calculs statistiques, le programme tente de faire une certaine classification du jeu données. Selon le besoin, le programme pourrait alors poser une étiquette sur des données non étiquetées similaires à celles retrouvées dans le jeu de donnée \DUrole{bibtex}{{[}bost\_machine\_2015 {]}}. Le modèle pourrait aussi être entrainer afin de reconnaître des anomalies, grouper des informations similaires par classes et bien d’autres \sphinxcite{zbib:noauthor-supervised-2020}. Toutes ces informations seront discutées plus en détails au courant de la prochaine section. Ce qu’il est important de retenir, c’est qu’entrainer un modèle nécessite \sphinxstyleemphasis{\sphinxstylestrong{beaucoup}} de données.

\begin{sphinxadmonition}{note}{Sur la signification de «beaucoup de données»}

Il y a 60 000 exemples dans nos données d’entraînement.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{In} \PYG{p}{[} \PYG{p}{]}\PYG{p}{:} \PYG{n}{train\PYGZus{}data}\PYG{o}{.}\PYG{n}{shape}
\PYG{n}{Out}\PYG{p}{[} \PYG{p}{]}\PYG{p}{:} \PYG{p}{(}\PYG{l+m+mi}{60000}\PYG{p}{,} \PYG{l+m+mi}{28}\PYG{p}{,} \PYG{l+m+mi}{28}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxadmonition}


\subsection{Traiter beaucoup de données}
\label{\detokenize{preprocessing:traiter-beaucoup-de-donnees}}
Pour reprendre l’exemple précédent, la \sphinxcode{\sphinxupquote{shape}} de l’objet \sphinxcode{\sphinxupquote{train\_data}}est une liste de 60 000 images représentées par des matrices carrées de dimension 28. Dans notre cas, si le programme passait tous les pixels un à un, il faudrait qu’il réalise séquentiellement \(28 \times 28 \times 60 000 = 47 040 000\) opérations. Ce serait par exemple le cas dans une \sphinxcode{\sphinxupquote{for loop}}. Bien que les ordinateurs modernes sont particulièrement rapides%
\begin{footnote}[18]\sphinxAtStartFootnote
Un ordinateur moderne possédant un processeur de 2GHz peut réaliser 2 000 000 000 opérations par seconde sur chacun de ses coeurs.
%
\end{footnote}, les modèles récents sont eux aussi entrainés avec des jeux de données de plus en plus massifs. Celui utilisé pour le service \sphinxhref{https://translate.google.ca}{Google Translate}, par exemple, compte des milliards d’exemples \DUrole{bibtex}{{[}noauthor\_size\_nodate {]}}.

Heureusement, il existe des méthodes permettant de paralléliser%
\begin{footnote}[19]\sphinxAtStartFootnote
Exécuter plusieurs opérations parallèlement plutôt que séquentiellement.
%
\end{footnote} les opérations statistiques réalisées sur notre modèle.


\subsubsection{Le parallélisme}
\label{\detokenize{preprocessing:le-parallelisme}}
Pour réduire le coût monétaire et temporel de l’entrainement d’un modèle, la tâche peut être séparée sur plusieurs des coeurs%
\begin{footnote}[20]\sphinxAtStartFootnote
Un «coeur» est une unité du processeur pouvant faire du calcul indépendamment des autres coeurs. Un processeur d’ordinateur portable moderne possède de deux à quatre coeurs. Une carte graphique moderne en possède quelques milliers, quoique moins performants que ceux du processeur.
%
\end{footnote} de la machine. Pour se faire, nous profiterons des propriétés des matrices.


\paragraph{Les propriétés des matrices}
\label{\detokenize{preprocessing:les-proprietes-des-matrices}}
Afin de trouver comment il serait possible de réaliser nos calculs en parallèle, analysons les propriétés des matrices.


\subparagraph{La multiplication}
\label{\detokenize{preprocessing:la-multiplication}}
Assumons les matrices de dimensions compatibles%
\begin{footnote}[21]\sphinxAtStartFootnote
Pour réaliser une multiplication entre deux matrices, il faut que le nombre de colonnes de la première matrice soit égal au nombre de rangées de la deuxième.
%
\end{footnote} \(A\) et \(B\):

\([A \times B]_{i,j} = \displaystyle\sum_{k=1}A_{i,k}B_{k,j}\)

Assumons aussi que la matrice \(C\) est produite par l’opération \(A \times B\) et que les matrices \(A\) et \(B\) sont carrées%
\begin{footnote}[22]\sphinxAtStartFootnote
Une matrice carrée est une matrice qui contient autant de rangées que de colonnes.
%
\end{footnote}.  Le calcul de \(C\) pourrait alors être implémenté de la manière suivante.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Initialisation de la matrice de départ.}
\PYG{c+c1}{\PYGZsh{} Nous assumons que les matrices sont 3x3.}
\PYG{n}{C} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
    \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
    \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}
\PYG{c+c1}{\PYGZsh{} Pour chaque rangée de la matrice A.}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
	\PYG{c+c1}{\PYGZsh{} Pour chaque valeur d\PYGZsq{}une rangée de la matrice B.}
	\PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{B}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
		\PYG{c+c1}{\PYGZsh{} Pour chaque rangée de la matrice B}
		\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{B}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
			\PYG{c+c1}{\PYGZsh{} [AxB]\PYGZus{}\PYGZob{}i,j\PYGZcb{} += A\PYGZus{}\PYGZob{}i,k\PYGZcb{} * B\PYGZus{}\PYGZob{}k, j\PYGZcb{}}
			\PYG{n}{C}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n}{A}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]} \PYG{o}{*} \PYG{n}{B}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}
\end{sphinxVerbatim}

Quoi qu’assez simple à implémenter, cette façon de calculer \(C\) est particulièrement inefficace. Alors que les matrices A et B augmentent en taille, le nombre d’opérations requises augmente…\sphinxstylestrong{au cube!} Si \(A\) passe d’une matrice \(2X2\) à une matrice \(3X3\), chaque \sphinxcode{\sphinxupquote{for loop}} doit être réalisée \(n\)%
\begin{footnote}[23]\sphinxAtStartFootnote
\(n\) représente le nombre de rangées et de colonnes d’une matrice carrée.
%
\end{footnote} fois de plus. Comme le programme contient 3 for loops imbriquées, si la première doit être faite \(n\) fois de plus, alors c’est de même pour la deuxième, puis la troisième. Le calcul est alors \(n \times n \times n = n^3\) fois plus complexe à réaliser \DUrole{bibtex}{{[}noauthor\_matrix\_2020 {]}}.

Heureusement, ce problème n’est pas sans issues. Reprenons l’équation de la multiplication de deux matrices.
\([A \times B]_{i,j} = \displaystyle\sum_{k=1}A_{i,k}B_{k,j}\)
Dans ce cas, chaque élément de \(C\) est produit par une sommation sur des multiplications d’éléments de \(A\) et \(B\). Il est aussi important de noter qu’aucun calcul pour un élément de \(C\) dépend d’un calcul pour un autre élément de \(C\)%
\begin{footnote}[24]\sphinxAtStartFootnote
Ils peuvent être calculés dans n’importe quel ordre et le résultat sera toujours le même. Un résultat pourrait aussi
%
\end{footnote}. Il serait donc possible de calculer plusieurs éléments de \(C\) en même temps!

Bien que le calcul en parallèle ne réduit pas l’ordre de complexité, il permet tout de même de diviser le temps requis par le nombre de coeurs utilisés%
\begin{footnote}[25]\sphinxAtStartFootnote
Plus ou moins. Voir \sphinxhref{https://en.wikipedia.org/wiki/Parallel\_computing\#Amdahl\%27s\_law\_and\_Gustafson\%27s\_law}{1.1 Amdahl’s law and Gustafson’s law}
%
\end{footnote}.


\subparagraph{L’addition}
\label{\detokenize{preprocessing:laddition}}
L’addition de deux matrices compatibles%
\begin{footnote}[26]\sphinxAtStartFootnote
Pour que deux matrices puissent être additionnées, elles doivent avoir le même nombre de rangées et de colonnes.
%
\end{footnote} se définit par l’addition de chacun des éléments homologues des deux matrices. Si nous reprenons les matrices carrées \(A\) et \(B\) utilisées plus haut, la somme de ces deux matrices serait:

\([A+B]_{i,j} = A_{i,j} + B_{i,j}\)

Supposons que la matrice \(C\) résulte de la somme de \(A\) et \(B\). Il est encore une fois possible d’affirmer que la valeur de \(C_{i,j}\) ne dépend pas de la valeur de \(C_{k,l}\). Il serait possible d’additionner chaque composante des deux matrices dans n’importe quel ordre en obtenant toujours le même résultat.

Encore une fois, l’addition de deux matrices peut être parallélisé afin de réduire le temps de calcul%
\begin{footnote}[27]\sphinxAtStartFootnote
En assumant que le calcul est réalisé sur une machine possédant plusieurs coeurs.
%
\end{footnote}.


\subparagraph{La multiplication par un scalaire}
\label{\detokenize{preprocessing:la-multiplication-par-un-scalaire}}
Bien que la multiplication par un scalaire s’avère facile à réaliser à la main pour de petites matrices, l’opération doit tout de même être réalisée sur chaque élément de la matrice.

\( \lambda \begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{d1} & x_{d2} & x_{d3} & \dots  & x_{dn}
\end{bmatrix} \)

Revient à faire le calcul:

\(\begin{bmatrix}
    \lambda x_{11} & \lambda x_{12} & \lambda x_{13} & \dots  & \lambda x_{1n} \\
    \lambda x_{21} & \lambda x_{22} & \lambda x_{23} & \dots  & \lambda x_{2n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    \lambda x_{d1} & \lambda x_{d2} & \lambda x_{d3} & \dots  & \lambda x_{dn}
\end{bmatrix}\)

Encore une fois, aucun résultat n’est dépendant d’un autre. Il serait donc possible d’effectuer plusieurs multiplications en même temps, puis grouper les résultats dans une matrice.


\subparagraph{La sommation}
\label{\detokenize{preprocessing:la-sommation}}
Nous avons ici un calcul légèrement différent des autres. Dans le cas de la sommation des éléments d’une matrice de dimension \((1,n)\), le calcul s’avère commutatif en plus d’être associatif%
\begin{footnote}[28]\sphinxAtStartFootnote
Dans le cas des autres opérations matricielles présentées, elles étaient seulement associatives. Les calculs étaient, comme pour la sommation, indépendants les un des autres. Par contre, pour les autres opérations les résultats devaient être placés de manière ordonnée dans la matrice résultante.
%
\end{footnote}. La commutativité de l’addition permet à notre programme d’utiliser l’opérateur de réduction.

\begin{sphinxadmonition}{note}{L’opérateur de réduction}

Un opérateur de réduction permet de réduire les éléments d’un \sphinxhref{https://fr.wikipedia.org/wiki/Tableau\_(structure\_de\_donn\%C3\%A9es)}{tableau} à un seul résultat. \DUrole{bibtex}{{[}wikireducop{]}}
\end{sphinxadmonition}

En premier lieu, voici comme une addition séquentielle d’un tableau pourrait être réalisé. Assumons un tableau de 8 entiers comme suit: \sphinxcode{\sphinxupquote{tableau = {[}2,9,6,4,1,3,8,8{]}}}. L’addition pourrait alors être réalisée en ajoutant chaque nombre un par un jusqu’à obtenir le total.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Création du tableau.}
\PYG{n}{tableau} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{9}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{8}\PYG{p}{,}\PYG{l+m+mi}{8}\PYG{p}{]}
\PYG{c+c1}{\PYGZsh{} Initiation de la variable `somme`.}
\PYG{n}{somme} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{c+c1}{\PYGZsh{} Pour chaque chiffre dans le tableau.}
\PYG{k}{for} \PYG{n}{chiffre} \PYG{o+ow}{in} \PYG{n}{tableau}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Sommation de l\PYGZsq{}ancienne somme avec le nouveau chiffre.}
    \PYG{n}{somme} \PYG{o}{+}\PYG{o}{=} \PYG{n}{chiffre}
\PYG{c+c1}{\PYGZsh{} Affichage de la somme.}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{somme}\PYG{p}{)}
\end{sphinxVerbatim}

Cet exemple permettrait de réaliser une sommation séquentielle sur tous les chiffres contenus dans le tableau. Ce reviendrait à réaliser le calcul suivant:
\((((((((2+9)+6)+4)+1)+3)+8)+8)\)
Bien que la moindre performance de cette méthode ne se fait pas ressentir pour des petites sommations, ce programme ne s’adapte pas bien à de grands tableaux%
\begin{footnote}[29]\sphinxAtStartFootnote
Voir la \{section\} sur les résultats concrets.
%
\end{footnote}.

Ensuite, si l’addition n’était qu’associative, l’opération pourrait tout de même être parallélisée. Les sommes partielles pourraient être calculées indépendamment les unes des autres comme pour les autres opérations matricielles. Le calcul serait similaire à celui ci:
\(((2+1)+(9+3))+((6+8)+(4+8))\)
Dans cet exemple, les sommes \(2+1\), \(9+3\), \(6+8\) et \(4+8\) sont calculées en même temps. Par contre, lorsque l’une des opérations est complétée avant une autre, il arrive que l’ordinateur ait à attendre \DUrole{bibtex}{{[}stackoverflowcommutativity{]}}. Le coeur ne pourrait alors pas se libérer pour faire d’autres opérations. Par exemple, assumons un ordinateur possédant deux unités de calcul disponibles et une addition non commutative. Si le calcul de \(2+9\) était complété avant celui de \(9+3\), l’ordinateur devrait attendre que les deux calculs soient complétés avant de calculer la somme partielle \((2+9)+(9+3)\). Heureusement, l’addition est associative. L’ordinateur ira donc écrire le résultat de la première opération complétée à la somme partielle, sans se soucier de la complétion de l’autre opération. L’unité de calcul sera alors libérée pour calculer, par exemple, la somme \(6+8\).

Finalement , l’opérateur de réduction est beaucoup plus adapté aux échelles de l’intelligence artificielle. Encore une fois, l’ordinateur sépare la sommation en plusieurs petites opérations qui peuvent être exécutés en parallèle. De plus, l’opérateur profite de l’associativité pour optimiser la tâche au maximum. À la fin de la réduction, il ne reste qu’une seule addition. Le calcul mathématique serait par contre identique au précédent.
\(((2+1)+(9+3))+((6+8)+(4+8))\)
Il est possible de remarquer que l’addition est fait dans un ordre particulier. Cet ordre donne un meilleur modèle d’accès à la mémoire.

Python utilise l’opérateur de réduction lors du calcul de sommations. Implémenter ce genre de solution s’avère donc assez simple.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tableau} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{9}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{8}\PYG{p}{,}\PYG{l+m+mi}{8}\PYG{p}{]}
\PYG{n}{somme} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{tableau}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{somme}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{NumPy}} possède aussi une fonction de sommation optimisée pour les \sphinxcode{\sphinxupquote{numpy arrays}}. Elle peut être implémentée tout aussi simplement.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{n}{tableau} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{9}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{8}\PYG{p}{,}\PYG{l+m+mi}{8}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{somme} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{tableau}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{somme}\PYG{p}{)}
\end{sphinxVerbatim}


\subparagraph{En bref}
\label{\detokenize{preprocessing:en-bref}}
En bref, une majorité des opérations matricielles peuvent être parallélisées. Les matrices sont donc la représentation de choix pour les jeux de données dans le domaine de l’intelligence artificielle. La transformation du jeu de données en matrices est une partie majeure du \sphinxstyleemphasis{preprocessing}. Elle permet d’accélérer le calcul d’un facteur non\sphinxhyphen{}négligeable.


\subsubsection{NumPy}
\label{\detokenize{preprocessing:numpy}}
C’est pour les opérations parallèles que la librairie \sphinxcode{\sphinxupquote{numpy}}, mentionnée lors de la {\hyperref[\detokenize{explications_librairies::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{section précédente}}}}, entre en jeu. Les opérations matricielles réalisées à l’aide de méthodes implémentés par \sphinxcode{\sphinxupquote{numpy}}profitent aussi de l’implémentation des \sphinxcode{\sphinxupquote{BLAS}}%
\begin{footnote}[30]\sphinxAtStartFootnote
\sphinxcode{\sphinxupquote{BLAS}} signifie \sphinxstylestrong{B}asic \sphinxstylestrong{L}inear \sphinxstylestrong{A}lgebra \sphinxstylestrong{S}ubroutines, ou sous\sphinxhyphen{}routines de base d’algèbre linéaire.
%
\end{footnote}. Les \sphinxcode{\sphinxupquote{BLAS}} permettent de grandement accélérer nos calculs sans même nécessiter de coeurs supplémentaires. Elles exploitent plutôt les différentes architectures de processeur ainsi que leur différents niveau de cache%
\begin{footnote}[31]\sphinxAtStartFootnote
Petite mémoire rapide allouée au processeur.
%
\end{footnote}.


\paragraph{\sphinxstyleliteralintitle{\sphinxupquote{BLAS}}}
\label{\detokenize{preprocessing:blas}}
Les sous\sphinxhyphen{}routines d’algèbre linéaire permettent de nettement réduire l’ordre de complexité des opérations d’algèbre linéaire. Elles permettent, par exemple, de décomposer des matrices en blocs afin d’accélérer la multiplication.

Ces sous\sphinxhyphen{}programmes sont extrêmement populaires. Ils sont implémentés dans une majorité des programmes de calcul scientifique \sphinxcite{zbib:noauthor-blas-nodate}.


\paragraph{Quelques résultats concrets}
\label{\detokenize{preprocessing:quelques-resultats-concrets}}
Voici quelques résultats plus concrets permettant d’obtenir une meilleure idée de l’ampleur de l’accélération des calculs.


\subparagraph{Multiplication de matrices à l’aide de \sphinxstyleliteralintitle{\sphinxupquote{NumPy}}.}
\label{\detokenize{preprocessing:multiplication-de-matrices-a-laide-de-numpy}}
Dans ce programme, deux matrices de dimensions \(1000 x 1000\) sont multipliées. La méthode de base avec les itérations imbriquées est comparée avec l’implémentation de l’opération par la librairie \sphinxcode{\sphinxupquote{NumPy}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{time}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{c+c1}{\PYGZsh{} Création de nos matrices}
\PYG{n}{A} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{,}\PYG{l+m+mi}{1000}\PYG{p}{)}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{,}\PYG{l+m+mi}{1000}\PYG{p}{)}
\PYG{n}{C} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{,}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Test de la première implémentation}

\PYG{n}{start\PYGZus{}time} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
	\PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{B}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
		\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{B}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
		\PYG{c+c1}{\PYGZsh{} 	print(A[i][k])}
\PYG{c+c1}{\PYGZsh{} 			print(B[k][j])}
\PYG{c+c1}{\PYGZsh{} 			print(i,j)}
			\PYG{n}{C}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n}{A}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]} \PYG{o}{*} \PYG{n}{B}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}
			
\PYG{n}{end\PYGZus{}time} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{time\PYGZus{}1} \PYG{o}{=} \PYG{n}{end\PYGZus{}time} \PYG{o}{\PYGZhy{}} \PYG{n}{start\PYGZus{}time}

\PYG{c+c1}{\PYGZsh{} Test de l\PYGZsq{}implémentation avec Numpy}
\PYG{n}{start\PYGZus{}time} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{C} \PYG{o}{=} \PYG{n}{A}\PYG{o}{*}\PYG{n}{B}

\PYG{n}{end\PYGZus{}time} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{time\PYGZus{}2} \PYG{o}{=} \PYG{n}{end\PYGZus{}time} \PYG{o}{\PYGZhy{}} \PYG{n}{start\PYGZus{}time}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Run time 1 = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{ seconds}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{time\PYGZus{}1}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Run time numpy = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{ seconds}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{time\PYGZus{}2}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Run} \PYG{n}{time} \PYG{l+m+mi}{1} \PYG{o}{=} \PYG{l+m+mf}{1838.9336512088776} \PYG{n}{seconds}
\PYG{n}{Run} \PYG{n}{time} \PYG{n}{numpy} \PYG{o}{=} \PYG{l+m+mf}{0.005700111389160156} \PYG{n}{seconds}
\end{sphinxVerbatim}

Alors que l’implémentation de base prend plus de 30 minutes à faire le calcul, \sphinxcode{\sphinxupquote{Numpy}} n’a besoin de que moins de 6 millièmes de secondes%
\begin{footnote}[32]\sphinxAtStartFootnote
Testé sur un processeur \sphinxstyleemphasis{2.9 GHz Dual\sphinxhyphen{}Core Intel Core i5}.
%
\end{footnote}!


\paragraph{Sommations de tableaux à l’aide de \sphinxstyleliteralintitle{\sphinxupquote{NumPy}}}
\label{\detokenize{preprocessing:sommations-de-tableaux-a-laide-de-numpy}}
Dans cet autre programme démontre quant à lui la différence entre notre implémentation de base de la sommation avec celle de \sphinxcode{\sphinxupquote{NumPy}} sur un \sphinxcode{\sphinxupquote{numpy array}}. La sommation se fait sur 1 milliard d’éléments aléatoires entre \(0\) et \(1\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{time}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{c+c1}{\PYGZsh{} Création du tableau et initiation de la somme.}
\PYG{n}{tableau} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{1000000000}\PYG{p}{)}
\PYG{n}{somme} \PYG{o}{=} \PYG{l+m+mi}{0}

\PYG{c+c1}{\PYGZsh{} Test de la première implémentation.}

\PYG{n}{start\PYGZus{}time} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Pour chaque chiffre dans le tableau.}
\PYG{k}{for} \PYG{n}{chiffre} \PYG{o+ow}{in} \PYG{n}{tableau}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Sommation de l\PYGZsq{}ancienne somme avec le nouveau chiffre.}
    \PYG{n}{somme} \PYG{o}{+}\PYG{o}{=} \PYG{n}{chiffre}
			
\PYG{n}{end\PYGZus{}time} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{time\PYGZus{}1} \PYG{o}{=} \PYG{n}{end\PYGZus{}time} \PYG{o}{\PYGZhy{}} \PYG{n}{start\PYGZus{}time}

\PYG{c+c1}{\PYGZsh{} Test de l\PYGZsq{}implémentation avec Numpy.}

\PYG{c+c1}{\PYGZsh{} Réinitialisation de la somme.}
\PYG{n}{somme} \PYG{o}{=} \PYG{l+m+mi}{0}

\PYG{n}{start\PYGZus{}time} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{somme} \PYG{o}{+}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{tableau}\PYG{p}{)}

\PYG{n}{end\PYGZus{}time} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{time\PYGZus{}2} \PYG{o}{=} \PYG{n}{end\PYGZus{}time} \PYG{o}{\PYGZhy{}} \PYG{n}{start\PYGZus{}time}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Run time 1 = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{ seconds}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{time\PYGZus{}1}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Run time numpy = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{ seconds}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{time\PYGZus{}2}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

La différence entre les résultats est encore une fois majeure.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Run} \PYG{n}{time} \PYG{l+m+mi}{1} \PYG{o}{=} \PYG{l+m+mf}{361.87627387046814} \PYG{n}{seconds}
\PYG{n}{Run} \PYG{n}{time} \PYG{n}{numpy} \PYG{o}{=} \PYG{l+m+mf}{5.082181215286255} \PYG{n}{seconds}
\end{sphinxVerbatim}


\subsection{Représenter des images}
\label{\detokenize{preprocessing:representer-des-images}}
Pour notre programme, l’intrant de notre réseau neuronal sera constitué d’images. Par contre, ces images ne seront pas directement passées au travers de notre programme dans leur format d’origine.

Comme discuté dans la section précédente, il serait préférable que les images soient représentées sous forme de matrices. C’est heureusement déjà le cas de nos données d’entrainement.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{In} \PYG{p}{[} \PYG{p}{]}\PYG{p}{:} \PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{train\PYGZus{}data}\PYG{p}{)}
\PYG{n}{Out}\PYG{p}{[} \PYG{p}{]}\PYG{p}{:} \PYG{n}{numpy}\PYG{o}{.}\PYG{n}{ndarray}
\end{sphinxVerbatim}

Dans cet exemple, \sphinxcode{\sphinxupquote{train\_data}} est un \sphinxcode{\sphinxupquote{array}} contenant l’ensemble de nos images. Pour obtenir le nombre d’éléments dans cet \sphinxcode{\sphinxupquote{array}}, la méthode \sphinxcode{\sphinxupquote{shape}} peut être utilisée.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{In} \PYG{p}{[} \PYG{p}{]}\PYG{p}{:} \PYG{n}{train\PYGZus{}data}\PYG{o}{.}\PYG{n}{shape}
\PYG{n}{Out}\PYG{p}{[} \PYG{p}{]}\PYG{p}{:} \PYG{p}{(}\PYG{l+m+mi}{60000}\PYG{p}{,} \PYG{l+m+mi}{28}\PYG{p}{,} \PYG{l+m+mi}{28}\PYG{p}{)}
\end{sphinxVerbatim}

La première valeur correspond au nombre d’image dans nos données d’entrainement. Les deux valeurs suivantes sont le nombre de rangées et de colonnes des matrices utilisées pour représenter ces mêmes images. Ce sont donc des matrices carrées de dimensions \(n = 28\).

Afin d’analyser précisément l’une de ces images, imprimons l’une des rangées de pixels.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{In} \PYG{p}{[} \PYG{p}{]}\PYG{p}{:} \PYG{n}{train\PYGZus{}data}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{20}\PYG{p}{]}
\PYG{n}{Out}\PYG{p}{[} \PYG{p}{]}\PYG{p}{:} \PYG{n}{array}\PYG{p}{(}\PYG{p}{[}  \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}   
                 \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}  
                 \PYG{l+m+mi}{24}\PYG{p}{,} \PYG{l+m+mi}{114}\PYG{p}{,} \PYG{l+m+mi}{221}\PYG{p}{,}  \PYG{l+m+mi}{253}\PYG{p}{,} \PYG{l+m+mi}{253}\PYG{p}{,} 
                 \PYG{l+m+mi}{253}\PYG{p}{,} \PYG{l+m+mi}{253}\PYG{p}{,} \PYG{l+m+mi}{201}\PYG{p}{,}  \PYG{l+m+mi}{78}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}
                 \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{,}
                 \PYG{l+m+mi}{0}\PYG{p}{,}   \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{uint8}\PYG{p}{)}
\end{sphinxVerbatim}

Les pixels sont représentés par des valeurs \sphinxstyleemphasis{grayscale} inversées. Traditionnellement, une valeur \sphinxstyleemphasis{grayscale} est élevée lorsque le pixel est très illuminé. La valeur maximale de \sphinxcode{\sphinxupquote{255}}signifie un blanc, alors que le \sphinxcode{\sphinxupquote{0}}correspond au noir. Dans notre jeu de données, ces deux valeurs sont inversées. Une valeur élevée signifie un pixel plus sombre.

Le paramètre \sphinxcode{\sphinxupquote{dtype=uint8}}signifie que nos pixels sont représentés par des entiers de 8 bits. Chaque bit ne pouvant avoir une valeur que de 1 ou 0, le plus grand nombre pouvant être représenté par ce type d’entier est 255.


\subsubsection{Explications plus détaillées sur la représentation des images.}
\label{\detokenize{preprocessing:explications-plus-detaillees-sur-la-representation-des-images}}
Quelques explications plus détaillées sur la représentation des images par des nombres.


\paragraph{Pourquoi le \sphinxstyleemphasis{grayscale}?}
\label{\detokenize{preprocessing:pourquoi-le-grayscale}}
Les couleurs ne s’avèrent pas très utiles \sphinxcite{zbib:noauthor-why-nodate} pour déceler les caractéristiques importantes d’une image. De plus, il nous faudrait beaucoup plus d’éléments dans une \sphinxcode{\sphinxupquote{array}} pour représenter des images en couleur qu’en noir et blanc. Par exemple, pour représenter une couleur en \sphinxcode{\sphinxupquote{RGB}}%
\begin{footnote}[33]\sphinxAtStartFootnote
Modèle de représentation des couleurs. Une couleur est représentée par les intensités des couleurs rouge, vert et bleu qui la constituent.
%
\end{footnote}, il nous faudrait 256 bits pour chacune des trois couleurs%
\begin{footnote}[34]\sphinxAtStartFootnote
Donc 768 bits.
%
\end{footnote}. En \sphinxstyleemphasis{grayscale}, seulement 256 bits sont nécessaires pour définir un pixel.


\subparagraph{Pourquoi est\sphinxhyphen{}il inversé?}
\label{\detokenize{preprocessing:pourquoi-est-il-inverse}}
Lors de notre entraînement, les valeurs de \sphinxcode{\sphinxupquote{0}}sont ignorées et ne nécessitent pas de calcul \sphinxcite{zbib:noauthor-impact-nodate}. Il est donc préférable d’avoir le plus de valeurs de \sphinxstyleemphasis{grayscale} à 0 possible. Les chiffres écrits à la main ne remplissent qu’une minorité de l’image. En utilisant les valeurs inversées, les pixels les moins nombreux ont une valeur de \sphinxcode{\sphinxupquote{256}}, alors que les plus nombreux ont une valeur de \sphinxcode{\sphinxupquote{0}}.


\bigskip\hrule\bigskip



\section{Notions de base}
\label{\detokenize{notions_de_base:notions-de-base}}\label{\detokenize{notions_de_base::doc}}

\subsection{Introduction au réseau neuronal}
\label{\detokenize{notions_de_base:introduction-au-reseau-neuronal}}
Un réseau neuronal est une forme d’intelligence artificielle, qui effectue des prédictions basées sur des valeurs qui sont entrées dans le système,
afin d’accomplir une certaine tâche. Le réseau est constitué d’un ensemble de neurones interconnectés et distribués en plusieurs couches.

Chaque neurone possède des paramètres qui peuvent être ajustés, afin d’obtenir des résultats plus fiables. C’est ce qu’on appele l’entrainement.
Le réseau est entrainé à partir d’un jeu de données, qui contient des valeurs associées à une étiquette, qui consiste de la « réponse » attendue.

Par exemple, un réseau neuronal ayant comme objectif de prédire l’achalandage dans un parc d’amusement pour une journée donnée pourrait recevoir
comme intrant la température, le niveau d’ensoleillement ainsi que le pourcentage de précipitation et d’humidité. Le jeu de données serait alors
constituée d’une liste ces quatres valeurs enregistrées à chaque jour des dernières années, avec comme étiquette le nombre de clients cette journée\sphinxhyphen{}là.
Les réponses du réseau sont comparées aux étiquettes, et les paramètres des neurones sont individuellement modifiés de manière à se rapprocher de la réponse attendue.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{resneuronalsimp}.png}
\caption{Ceci est un exemple simplifié d’un réseau neuronal. Les composantes du schéma seront expliquées en détail dans cette section.}\label{\detokenize{notions_de_base:reseau-neuronal}}\end{figure}


\subsection{OCR}
\label{\detokenize{notions_de_base:ocr}}
Le terme OCR, ou ROC en français, signifie « Reconnaissance optique de caractères ». Cela désigne un processus aucours duquel du texte est extrait
d’une image ou d’un document afin d’être transformé en fichier. Pour ce faire, un réseau neuronal reçoit les valeurs des pixels du document de source,
\begin{quote}

Note : La valeur d’un pixel en « grayscale » ou échelle de gris, est un nombre entier
de format 8 bits et peut donc avoir
une valeur comprise entre 0 et 255 (2\textasciicircum{}8 \sphinxhyphen{} 1), où 0 est noir et 255 est blanc.
Un pixel en couleur est représenté sous la forme d’un vecteur de 3 nombres 8
bits, chaque nombre correspondant à une valeur de rouge, vert et bleu. \sphinxcite{zbib:hipr2}
\end{quote}

traitées afin de les rendre utilisables par le réseau. Ces données se propagent ensuite vers l’avant
dans le réseau, de couche de neurone en couche de neurone, avant d’aboutir à la couche d’extrants, composée de 10 neurones dans le cas de notre
programme, qui correspondent aux chiffres de 0 à 9. Un de ces neurones de cette couche finale s’active, donnant ainsi le résultat estimé par le réseau.
Ensuite, divers paramètres sont ajustés par un algorithme d’optimisation afin d’augmenter la précision des réponses du réseau.


\subsection{Le neurone}
\label{\detokenize{notions_de_base:le-neurone}}
Le neurone est l’unité de base d’un réseau neuronal. C’est un noeud parmis le réseau par lequel transitent des valeurs, qui sont modifiées
au passage par un procédé qui sera expliqué plus en détail prochainement, avant d’être envoyées vers les prochains neurones.
Essentiellement, un neurone reçoit une ou des valeurs comme intrant, effectue des opérations sur ces dernières, puis transmet la nouvelle valeur.

La structure d’un neurone est relativement simple. Chaque neurone possède un coefficient, ou un \sphinxstylestrong{poids} \(p\) dans le jargon, associé à chaque \sphinxstylestrong{intrant} \(I\) qu’elle reçoit.
La première opération que la neurone effectue est la somme des produits des intrants fois leur poids. À celà est ajouté un \sphinxstylestrong{biais} \(b\) propre à chaque neurone.
Cette opération peut être représentée par la fonction \(Y = \sum_{i=1}^{n} I_i \times p_i + b\), où n correspond au nombre d’intrants.

La dernière opération que les valeurs subissent avant d’être transmises est une fonction d’activation. La fonction d’activation est appliquée à chaque extrant de chaque
neurone de la couche. Les fonctions d’activation, analogues à l’activation
d’un neurone biologique, permettent généralement d’obtenir un extrant compris entre 0 et 1, ou \sphinxhyphen{}1 et 1. Elles ont plusieurs utilités, notamment
pour la modélisation de fonctions non linéeaires, ainsi que pour l’entrainement du réseau, ce qui sera expliqué dans une section ultérieure.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{neurone}.png}
\caption{Exemple des opérations effectuées au sein d’un neurone.}\label{\detokenize{notions_de_base:neurone}}\end{figure}

La fonction la plus simple est la fonction à échelons. Elle retourne 1 si l’intrant \sphinxstyleemphasis{x} est plus grand qu’une valeur seuil \sphinxstyleemphasis{s}, et 0 s’il ne l’est pas. Cette fonction peut être représentée par l’équation
\(
E(x)=
\begin{cases}
 1 & \quad \text{si } x \text{ > s}\\
 0 & \quad \text{si } x \text{ <= s}
\end{cases}
\)
Elle n’est néanmoins pas utilisée, puisqu’elle empêche l’entrainement du réseau.
La fonction d’activation doit être dérivable en une autre fonction, et non en une constante, afin que le processus d’ajustement des paramètres puisse avoir lieu.
Il est également impossible de représenter des situations non\sphinxhyphen{}linéeaires avec cette fonction, puisque seulement des fonctions linéaires sont présentes dans le réseau.

La fonction d’activation la plus utilisée est la fonction Unité Linéaire Rectifiée, ou « ReLU » en anglais (Rectified Linear Unit).
Cette fonction peut être représentée par l’équation :  \(
R(x)=
\begin{cases}
 x & \quad \text{si } x \text{ > 0}\\
 0 & \quad \text{si } x \text{ <= 0}
\end{cases}
\)

ou encore, \( R(x) = max(0, x)\). Cette fonction est peu demandante à calculer pour l’ordinateur, et se fait très rapidement. De plus, malgré son apparence linéaire,
elle peut être dérivée, ce qui est nécessaire pour pouvoir entrainer le réseau. C’est pour ces raisons que c’est la fonction d’activation la plus répendue.
Elle a toutefois comme désavantage de produire parfois une trop grande quantité de « 0 », ce qui peut entrainer une réaction en chaine, où ces zéros se propagent,
empêchant le bon fonctionnement du réseau. Cette situation est appelée la « mort du réseau », où l’extrant de plusieurs neurones devient invariablement 0, ce qui
diminue l’efficacité du réseau. Ce phénomène se produit surtout lorsque le réseau se fait entrainer de manière trop rigoureuse, et que le biais de certaines
neurones devient une très grande valeur négative, ce qui fait que l’intrant dans la fonction d’activation est toujours en dessous de 0, et l’extrant reste ainsi
invariablement 0.

Une variation de cette fonction, nommée Leaky ReLU, a été créée afin de tenter de régler ce problème de mort du réseau : \( 
L(x)=
\begin{cases}
 x & \quad \text{si } x \text{ > 0}\\
 0,01 \times x & \quad \text{si } x \text{ <= 0}
\end{cases}
\)

Ici, les zéros sont remplacés par de très petits nombres négatifs, qui correspondent généralement à x multiplié par le coefficient 0,01.

Une autre fonction commune est la sigmoide. Son équation est :
\( \phi(x) = 
\frac{1}{1 + e^{-x}}
\)
La fonction retourne 0 lorsque x tend vers l’infini négatif, et 1 lorsque x tend vers l’infini positif. Cette fonction a comme avantage de
s’approcher rapidement de 0 ou de 1, lorsque l’intrant \sphinxstyleemphasis{x} est plus petit que \sphinxhyphen{}2 ou plus grand que 2, respectivement. Cela permet d’envoyer
un signal très fort aux prochains neurones. Cela peut toutefois devenir un désavantage lorsque les intrants sont très grands, puisque l’extrant
reste pratiquement le même, ce qui peut nuire à l’entrainement. Cette fonction est également plus lourde pour l’ordinateur, ce qui peut ralentir
considérablement le système lorsque ce calcul est effectué des centaines ou des milliers de fois.

Une fonction similaire à la sigmoide et la TanH. Son équation est :
\( tanh(x) = 
\frac{2}{1 + e^{-2x}} - 1
\)
Elle retourne \sphinxhyphen{}1 lorsque x tend vers l’infini négatif, et 1 lorsque x tend vers l’infini positif. Elle a comme avantage de retourner en moyenne
des valeurs proches de 0, ce qui rend la tâche plus facile pour les couches suivantes, puisque les valeurs auront moins tendance à devenir très grandes,
ce qui ralentirait les opérations.


\subsection{Couches de neurones}
\label{\detokenize{notions_de_base:couches-de-neurones}}
Comme mentionné précedemment, les neurones sont organisés en couches. Il y a 3 types de couches différentes. La première est la couche des intrants, dans laquelle
les données sont rentrées dans le réseau. Dans le cas de notre programme, où les intrants sont des images de format 28x28,
la première couche est composée de 784 (\(28\times28 = 784\)) neurones recevant chacun la valeur en échelle de gris d’un pixel de l’image.
Plus concrètement, ces images sont des matrices carrées \(M_{28}\), qui se font vectoriser en un vecteur de taille 784. Par la suite, chacune de ces
données est transmise à chacun des neurones de la couche cachée, puisque le réseau est densément connecté, et les neurones d’une couche sont connectés à
tout ceux des couches adjacentes. Pour la suite de cette explication, le réseau neuronal provenant de la figure affichée plus haut sera utilisé, à des
fins de clarté. Donc, les valeurs des trois neurones de la couche d’intrants sont contenus dans la matrice \(I_{1\times3}\). Les poids des neurones de la couche cachée 1 sont
contenus dans la matrice \(C_{4\times3}\), où 4 correspond au nombre de neurones dans la couche, et 3 aux poids que possèdent chaque neurones de la couche (un poid par neurone
de la couche précédente). Ici, l’opération à faire serait un produit matriciel
\(A_{m\times p} \times B_{p\times n} = C_{m\times n}\)
, afin de multiplier les intrants par chaque ensemble de poids. Toutefois, les matrices ne sont
pas compatibles pour effectuer cette opération, puisque le nombre de colonnes de la première matrice n’est pas égal au nombre de rangées de la seconde.
Il faut donc faire la transposée de la matrice \(C_{4\times3}\), qui devient alors \(C_{3\times4}^{t}\). L’opération \(I_{1\times3} \times C_{3\times4}^{t}\), où sont multipliés
dans l’ordre, élément par élément, chaque élément d’une ligne de \sphinxstyleemphasis{I} par chaque élément d’une colonne de \sphinxstyleemphasis{C}, puis est effectué la somme de
ces produits pour obtenir un nouvel élément de la matrice résultante \(R_{1\times4}\) \DUrole{bibtex}{{[}Alloprof{]}}. Par la suite, la matrice \(B_{1\times4}\)
contenant les biais de chaque neurone
de la couche est additionée à la matrice R, dans une opération où s’additionnent entre\sphinxhyphen{}eux les éléments correspondants de chaque matrice pour
former une nouvelle matrice de même dimension. Finalement, dans une itération au travers de cette matrice, chaque élément passe par la fonction d’activation,
pour former encore une nouvelle matrice de même dimensions contenant les résultats de cette dernière opération. Cette matrice résultante finale \(F_{1\times4}\) devient
alors l’intrant de la couche suivante de neurones, et ainsi de suite.


\subsection{Réseaux neuronaux et le cerveau humain}
\label{\detokenize{notions_de_base:reseaux-neuronaux-et-le-cerveau-humain}}
Plusieurs liens peuvent être faits entre les réseaux neuronaux et le cerveau humain. Le premier réseau neuronal était un
système mécanique financé par la marine américaine qui tentait d’émuler les neurones biologiques. La fonction d’activation
à échelons était utilisée, imitant les neurones biologiques qui s’activent \sphinxstyleemphasis{1} ou ne s’activent pas \sphinxstyleemphasis{0}. Le projet a rapidement
été laissé de côté, principalement à cause du fait que le réseau était extrêmement difficile à entrainer, puisque, comme vu plus
tôt, la fonction à échelon ne permet pas l’entrainement du réseau, et les paramètres devaient être ajustés au hasard.

Voilà donc une première différence fondamentale entre les neurones artificiels et organiques. Les neurones artificiels peuvent
sortir toutes sortes de valeurs, de manière à mieux servir les intérêts du système, alors que dans le cas d’un neurone organique,
elles ne peuvent envoyer que le signal binaire \sphinxstyleemphasis{activé} ou \sphinxstyleemphasis{non\sphinxhyphen{}activé}.
\begin{quote}

Un neurone s’active lorsque son seuil d’excitation est atteint. Le potentiel de repos d’un neurone est d’environ \sphinxhyphen{}50mV. Lorsqu’il
reçoit suffisament de neurotransmetteurs (des particules envoyées par d’autre neurones et qui possèdent une charge électrique), par
ses dendrites et que le seuil d’excitation d’environ 15mV est atteint, le potentiel d’action se déclenche, et un influx nerveux se propage
le long de l’axone sous forme de courant électrique. Une fois arrivé aux terminaisons axonales du neurone, d’autres neurotransmetteurs sont
libérés par les synapses, poursuivant ainsi la transmission du signal. La quantité de neurotransmetteurs libérée ne dépend pas de l’intensité
du stimulus initial ; c’est une situation de tout ou rien. \sphinxcite{zbib:futura-sciences}
\end{quote}

En d’autres termes, l’image de la fonction d’un neurone artificiel \sphinxstyleemphasis{A}, dépendamment de la fonction d’activation, peut être,
par exemple \(\mathbb{R}\) , \(\mathbb{R^+}\), ou encore \([-1, 1]\),
tandis que l’image de la fonction d’un neurone organique \sphinxstyleemphasis{O} est toujours limité à \(\text{{0, 1}}\).

L’aspect où les réseaux neuronaux et le cerveau humain ont le plus en commun est leur état initial. Les deux commencent comme un canvas vierge, ne possédant
aucune connaissances ou expériences. Les deux se font « entrainer » par des informations extérieurs, jusqu’à arriver au point ou ils deviennent autonomes.
Les connaissances qu’ils amassent se trouvent d’une certaine manière encodées dans leur système, et influencent leurs actions futures.


\section{L’entrainement d’un système neuronal}
\label{\detokenize{training:l-entrainement-d-un-systeme-neuronal}}\label{\detokenize{training::doc}}
L’entrainement d’un réseau à l’aide d’une certaine base de données (donnée d’entrainement) permet à celui\sphinxhyphen{}ci de prédire le résultat
d’une autre base donnée. En effet, le but d’un réseau neuronal est de réduire l’erreur de l’entrainement ainsi que la différence
entre l’erreur des données entrainées et l’erreur des données de test soient petites. Lorsque le réseau est sous\sphinxhyphen{}entrainé,
le réseau de sera pas précis lors de ces résultats. Cependant, lorsque le réseau est sur\sphinxhyphen{}entrainé, celui\sphinxhyphen{}ci va prendre en compte
tout le bruit des données. Ce bruit peut être, par exemple, le fait de prendre en compte les imperfections d’une image, reconnaitre
seulement certains styles d’écriture, etc. Cela a comme impact d’augmenter l’erreur lorsque le système est exposé à une nouvelle base de données.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{overfitting}.png}
\caption{Graphiques représentant l’effet de l’entrainement du réseau de neurone}\label{\detokenize{training:overfitting}}\end{figure}

L’entrainement d’un réseau neuronal s’effectue à l’inverse. Visuellement, l’entrainement et l’ajustement des différents
paramètres se font de la droite vers la gauche. Ce principe, appelé « backpropagation », va être expliqué à l’aide quelques
démonstrations mathématiques complémentées par quelques explications écrites.


\subsection{Fonction d’erreur}
\label{\detokenize{training:fonction-d-erreur}}
Une fonction d’erreur est une fonction permettant de connaitre la précision des résultats des extrants de la dernière
couche. Il peut y avoir plusieurs fonctions d’erreur. En voici un exemple:

\(E_{SS}=1/2\sum_{i=1}^nE_i^2 \) \sphinxstylestrong{(1.1)}

\(E_{SS}=1/2\sum_{i=1}^nE_i^2 \) \sphinxstylestrong{(1.2)}

où \(E_{SS}\)= « error sum of square ». Cela est tout simplement une de plusieurs fonctions d’erreur.

\(E_i =|{t_i-I_i}|\) \sphinxstylestrong{(1.3)}

où \(E_i\) correspond à l’erreur d’un neurone de la dernière couche (extrant). \(I_i\) correspond à la valeur numérique
d’un extrant et \(t_i\) correspond à la valeur désirée provenant de la base de données fournies.

Combiner les deux équations permet d’obtenir:

\(E=1/2\sum_{i=1}^n({T_i-Y_i})^2\) \sphinxstylestrong{(1.4)}


\subsection{Transmition de l’information}
\label{\detokenize{training:transmition-de-l-information}}\begin{quote}

Note: Afin de simplifier les explications, ces dernières seront faites en utilisant un réseau neuronal ayant seulement 1 neurone par couche.
\end{quote}

D’abord, il faut comprendre comment le réseau transmet son information de cellules en cellule. En effet,
un neurone ayant contenant une certaine valeur \(Y\) transmet cette dernière à tous les autres neurones de
la prochaine couche. Cependant, ces transmitions n’ont pas toutes les mêmes poids. Ces poids \(p\) diffèrent
afin de favoriser certaines activations et en défavoriser d’autres. Chaque liaison entre chaque neurone possède
un poid propre à chacune. Ces derniers sont multipliés avec l’extrant de la neurone en précédentes.

\(Y_{i} = Y_{i-1}\times p_{i}\)\sphinxstylestrong{(2.0)}

où \(p_{i}\) correspond au poid de la neurone de la couche i

Ensuite, un biais \(b\) est additionné ou soustrait au résultat précédent

\(Y_i = Y_{i-1}\times p_{i} + b_i\) \sphinxstylestrong{(2.1)}

d’activation sera expliqué en détail plus loin.où \(b_i\) correspond au biais de la neurone de la couche i.

Finalement, une fonction d’activation \(a\) est ajoutée au reste de la formule. L’utilité et le fonctionnement de
la fonction d’activation sera expliqué en détail plus loin.

\(Y_i = a\times(Y_{i-1}\times p_{i} + b_i)\)
\sphinxstylestrong{(2.2)}


\subsection{\sphinxstyleemphasis{Back propagation}}
\label{\detokenize{training:back-propagation}}
L’objectif est de comprendre comment le poids et le biais doit être ajuster en débutant de la fonction d’erreur et d’activation.

Dabord, en utilisant la formule de base de transmission d’un neurone (sans le biais) :

\(Y = \sum_{i=1}^{n} I_i \times p_i \)

Il est possible de comprendre comment le changement d’une variable impact une autre. Les dérivés seront
donc utilisées afin de démontrer ce principe.

\(\frac{dY}{dI_i}=\frac{dY}{dI_i}\sum_{i=1}^{n} I_i \times p_{ji} 
\)

\(\frac{dY}{dI_i} = p_i\)

\(\frac{dY}{dp_i} = I_i\)

Cela veut donc dire que le poid influence le résultat de l’extrant et que l’intrant influence
le résultat de l’extrant.

En utlisant la formule (1.4) et le concept de dérivée partielle, il est possible de comprendre
l’impact d’un changement de la valeur de l’intrant \(I_i\) sur l’erreur:

\(\frac{dE}{dI_i} =  (2/2)(t_i - I_i)(-1) \)
\(\frac{dE}{dI_i}= -(t_i-I_i)\)

Maintenant, il faut calculer la dérivation de la fonction d’activation.
La fonction sigmoïde sera utilisée pour cet exemple.

\(a = \frac{1}{1 + e^{-Y}} =(1+e^{-Y})^{-1}\)

\(\frac{da}{dY} = -1 (-e^{-Y})(1+e^{-Y})^{-2} \)

\(= \frac{e^{-Y}}{(1+e^{-Y})^2} \)

\(= \frac{1}{(1+e^{-Y})}\times\frac{e^{-Y}}{(1+e^{-Y})} \)

\(= a \times \frac{e^{-Y}}{(1+e^{-Y})}\)

\(= a \times \frac{1+e^{-Y}-1}{(1+e^{-Y})} \)

\(= a \times (\frac{(1+e^{-Y})}{(1+e^{-Y})} + \frac{-1}{(1+e^{-Y})})\)

\(\frac{da}{dY}= a \times (1-a)\)

Maintenant il est possible, à l’aide de la règle de dérivation en chaine, de trouver l’impact
qu’a \(Y\) sur l’erreur \(E\). Dans cet exemple, \(I_i = a \) puisque la fonction d’activation été appliquée au neurone en question.

\(\frac{dE}{dY_i} = \frac{dE}{dI_i} \times \frac{dI_i}{dY_i}\)

\(= \frac{dE}{dI_i} \times \frac{da}{dY_i}\)

\(=-(t_i - I_i)  I_i (1- I_i)\)  \sphinxstylestrong{(3.0)}

Ensuite il est possible de calculer la dérivation de l’erreur en fonction du poid \(p_{ji}\) d’une liaison entre deux neurones.

\(\frac{dE}{dp_{ji}} =\frac{dE}{dY_i} \times \frac{dY_i}{dp_{ji}} \)

\(= (-(t_i - I_i) \times I_i\times (1- I_i))\times I_i\)

\(= -I_i I_j (1-I_i)(t_i-I_i)\)\sphinxstylestrong{(4.0)}

Cette équation signifie que le changement de l’erreur influence le poid et cette influence
correspond à l’extrant d’un neurone négatif multiplié par l’extrant du neurone précédent et
ce tout est multplié par 1 moins la valeur du neurone. À ce résultat est multiplié la valeur
de l’erreur soit : \((t_i-I_i)\)
L’équation 3.0 sera représenté par la variable:  \(\Delta p\)

Le concept de « backpropagation » se résume donc a:

\(p_{ji} = p_{ji} + \Delta p\)

Le poids d’un neurone change légèrement en additionnant un \(\Delta p\)  positif ou négatif. Ce changement
est fait avec une plus grande importance plus le neurone est proche de la couche des extrants. Cela est
dû au fait que l’apprentissage commence par la couche finale pour enfin se rendre jusqu’à la couche débutant
le système neuronal. Les premiers ajustements, donc ceux des couches plus proches de la fin, sont plus importants.
Les couches se situant plus au début du réseau vont plutôt avoir de petits changements à leur poids puisque
rendu à l’ajustement de dce dernier, l’erreur est déja considérablement réduite. Ce concept se nomme descente
de gradient stochastique.

\#\#\#Origine du biais
Certains problèmes peuvent survenir avec le concept de descente de gradiant dans un réseau neuronal.
En effet, lorsqu’une couche « n’apprend plus » ou, en d’autres mots, lorsque le poids ne varie plus,
on assiste a un problème se nommant la disparition du gradiant. Cela est un problème pour le réseau puisque
le tout l’entrainement se fait uniquement dans les dernières couches. Un autre problème est que le gradiant
dans une fonction de coût telle une sigoïde, le gradient se situe uniquement au milieu de la fonction comme
le montre le graphique ci\sphinxhyphen{}dessous.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{tanh}.png}
\caption{Fonction sigmoïde}\label{\detokenize{training:tanh}}\end{figure}

En effet, les extrémités de la fonction forment un plateau. Il n’y a donc pas de changement
possible puisque soit l’intrant est multiplié par 1, ce qui ne change pas le résultat, ou bien
soit le résultat est multiplié par 0 ce qui rend la valeurr nul et cela est néfaste pour un réseau neuronal.
En effet, une valeur égale a 0 empêche l’entrainement du réseau puisque peut importe la variation
du poid, \(0\times p\) sera toujours être égale à 0. C’est pour remédier à cette erreur qu’un biais est ajouté
à la fonction.

\(Y =\sum_{i=1}^{n} I_i \times p_i + b\)

L’ajout de ce biais va permettre de conserver un apprentissage même lorsque la valeur d’un neurone est figée à 0.


\section{Bienfaits et inconvénients}
\label{\detokenize{bienfaits_et_inconv_xe9nients:bienfaits-et-inconvenients}}\label{\detokenize{bienfaits_et_inconv_xe9nients::doc}}
Il est important de parler des impacts positifs et négatifs de l’intelligence
artificielle dans le but de bien comprendre ce que cette technologie peut apporter
à l’évolution de la société. C’est pour cela que cette section va répondre à la
sous\sphinxhyphen{}question de la thèse: Est\sphinxhyphen{} ce que l’intelligence artificielle peut constituer
un bénéfice pour l’être humain.

Pour commencer, l’intégration de l’intelligence artificielle à notre vie de tous
les jours ainsi que dans plusieurs tâches pourrait amener plusieurs bénéfices
importants.

Qui dit programme, dit souvent un taux d’erreur qui diminue. En effet, l’erreur
humaine est présente et joue un gros rôle dans plusieurs métiers. L’erreur est humaine
, mais minimiser l’effort tout en maximisant la précision est primordial pour
certaines tâches. L’utilisation d’une intelligence artificielle bien entraînée
permet de réduire le taux d’erreurs et le temps requis à l’exécution d’une tâche.
Cela est possible, car un programme assisté par l’intelligence artificielle peut
parcourir un large jeu de données et appliquer des algorithmes en peu de temps,
tandis que l’être humain prendrait des mois et des mois pour faire cela. Un exemple
moderne serait l’utilisation de l’intelligence artificielle dans les prévisions météo.
Ceux\sphinxhyphen{}ci deviennent donc de plus en plus précis avec l’aide de l’apprentissage
machine ainsi que l’amélioration des algorithmes.

L’IA peut assister l’être humain dans des tâches dangereuses qui pourraient risquer
la vie d’individus, mais qui est sans risques pour l’intelligence artificielle.
Un exemple serait le robot \sphinxhref{https://mars.nasa.gov/msl/home/}{Curiosity} lancé en novembre 2011 dans le but d’atteindre
la planète Mars. Il se trouve toujours sur cette planète en date du 19 novembre
2020 dans des conditions impossibles pour qu’un être humain y soit resté aussi
longtemps. Celui\sphinxhyphen{}ci est contrôlé par l’intelligence artificielle dans le but
d’accomplir plusieurs tâches. Ses tâches consistent à examiner la biologie, la
géologie  et la radiation de Mars, afin de préparer une exploration humaine dans le futur.

Les conditions de travail standard dictent qu’un être humain travaille environ
40 heures par semaine en incluant les pauses. De plus, ce ne sont pas toujours
40 heures productives. Les fins de journées et les fins de semaine sont souvent
peu productives. En comparaison, un ordinateur n’a pas besoin de pause et peut
travailler 24 heures sur 24, 7 jours sur 7. L’intelligence artificielle sera
sûrement plus efficace qu’une majorité des travailleurs  et  aussi plus constante
dans sa vitesse d’exécution. Cet ennui est souvent causé souvent par des travaux
répétitifs. Ces tâches banales peuvent être automatisées par l’IA. Donnant plus de
temps aux travailleurs pour se concentrer sur des tâches plus exigeantes et passionnantes.
Comme les ordinateurs le font déjà dans plusieurs domaines, tel celui de la finance
où un apprentissage machine s’occupe de détecter des fraudes potentielles dans
les transactions des clients.

Quand on parle de rapidité, l’être humain est très lent comparé à l’intelligence
artificielle. En effet, l’utilisation de l’intelligence artificielle pour la prise
de décision permet de prendre ceux\sphinxhyphen{}ci extrêmement rapidement avec une précision qui
dépend de la qualité de leurs bases de données. Effectivement, un jeu de données fiable
et bien construit permet à l’intelligence artificielle de prendre des bonnes décisions
rapidement. Un être humain, de l’autre côté, doit analyser une plus petite quantité de
données directement sous son nez en prenant plus de temps. Un des exemples serait
l’utilisation de l’intelligence artificielle dans les jeux d’échecs. L’intelligence
artificielle à accès à des milliers et des milliers de parties dans son jeu de
données. Alors, lorsque son adversaire humain joue le coup  Nc3 (Cavalier sur la case C3),
l’ordinateur analyse toutes les parties où ce coup c’est fait et en ressort le meilleur
coup pour contrer celui\sphinxhyphen{}ci, et ce, en un temps record. De plus, la prise de décision de
l’intelligence artificielle ne va pas seulement s’arrêter au jeu d’échec. Nous voyons
déjà l’industrie médicale se faire aider dans la prise de décision par l’intelligence
artificielle en plus des nouveaux programmes qui se font développer en ce moment. Ceux\sphinxhyphen{}ci
ont pour but d’aider les professionnels de la santé dans des diagnostics, surveillances
de patients, et plusieurs autres. La compagnie Google est en train de développer  leur
projet \sphinxhref{https://health.google/}{Google Health} Pour se faire, ils étudient l’utilisation de l’intelligence artificielle
dans le but d’assister au diagnostic de cancer, prévenir l’aveuglement et plusieurs autres.

La croissance exponentielle de l’intelligence artificielle dans le monde a affecté
de manière abrupte le nombre de travail qui requiert des capacités en intelligence
artificielle, et donc, a créé plusieurs nouveaux emplois dans ce domaine. Grâce à cela,
la part des emplois qui requièrent des capacités en intelligence artificielle a augmenté
de 4,5 fois depuis 2013 aux États\sphinxhyphen{}Unis sur le site web \sphinxhref{https://ca.indeed.com/?r=us}{indeed.com}.

Certes, ces merveilleux bénéfices ne viennent pas sans coût. Or, l’intelligence artificielle
s’avère à être une arme à double tranchant et continents plusieurs problèmes

Un de ces  problèmes majeurs est le biais de l’intelligence artificielle intégrée dans
le modèle de manière involontaire ou intentionnelle par la façon dont celui\sphinxhyphen{}ci est entraîné
ou programmé. Un exemple flagrant serait le programme Correctional Offender Management
Profiling for Alternative Sanctions (\sphinxhref{https://en.wikipedia.org/wiki/COMPAS\_(software)}{COMPAS}) qui a signalé à tort des personnes ayant la
peau foncée presque deux fois plus que des personnes ayant la peau blanche(45 \% à 24 \%)
devant la cour juridique aux États\sphinxhyphen{}Unis. Un autre exemple serait l’intelligence artificielle
programmée par Microsoft Corporation dans le but d’interagir avec les internautes.
Prénommé «\sphinxhref{https://twitter.com/tayandyou?lang=en}{Tay}», le bot s’est rapidement fait retirer après un entraînement avec l’interaction
d’internautes racistes. Cela a mené le robot à dire des phrases vulgaires et inappropriées
avant que celui\sphinxhyphen{}ci voit la fin de ses jours 16 heures après son arrivée sur Twitter.
Le problème provient donc d’un jeu de données non fiable où mal construit qui induit
l’apprentissage machine en erreur.

Tel que mentionné plus haut, l’intelligence artificielle a apporté et va apporter plusieurs
nouveaux emplois spécialisés dans l’intelligence artificielle, mais plusieurs autres emplois
vont voir leur fin arriver à grands pas avec la montée de l’intelligence artificielle. Cela
c’est déjà fait avec la révolution industrielle, qui, avec l’arrivée de la machine à vapeur,
avait fait grimper le taux de chômage à une vitesse fulgurante. L’intelligence artificielle
devrait, tout comme la machine à vapeur, augmenter le besoin de certains emplois et créer des
emplois qui requièrent un plus haut niveau d’étude en éducation tout en éradiquant des emplois
auxquels aucune caractéristique uniquement humaine est requise. Il reste à noter que l’être
humain s’en est quand même bien sortie de la révolution industrielle, contrairement aux idées
pessimistes quant au futur proliférées à l’époque.

Par ailleurs, l’omniprésence de la technologie a créé une multitude d’interconnexions entre
tous les pays. En plus, avec l’implémentation de l’intelligence artificielle, chaque pays va
définir des lois et des règlements sur l’intelligence artificielle.
Dans un monde idéal, tous les êtres humains s’entendent sur les mêmes lois et règlements pour
éviter des conflits avec d’autres pays. Ces conflits seraient à cause de décisions par rapport
à l’intelligence artificielle qu’un pays a choisi et qui mène à une contradiction avec un ou
plusieurs autres pays. Malheureusement, comme l’Histoire nous l’a si bien démontré, essayer
d’établir des règles communes s’avère difficile, et de les faire respecter, encore plus. Des différences
se font déjà voir entre les puissances du monde face à l’intelligence artificielle avec l’Union
Européenne qui est déjà entrain de pousser pour des mesures plus strictes pour le développement et
l’utilisation de l’intelligence artificielle avec le ‘White Paper on Artificial Intelligence \textendash{}
A European Approach to Excellence and Trust’ ,publié en 2020. Au contraire, les États \sphinxhyphen{} Unis
et la Chine permettent à leur compagnie d’utiliser l’intelligence artificielle plus librement.

L’intelligence artificielle va augmenter l’efficacité et la rapidité de plusieurs programmes.
Cela n’exclut pas les programmes de piratages informatiques, qui eux aussi vont voir une
amélioration drastique avec l’implémentation de l’intelligence artificielle. Cela va donc
augmenter aussi la rapidité et l’efficacité des piratages informatiques menant sûrement à
une augmentation de ceux\sphinxhyphen{}ci ce qui causera plusieurs problèmes majeurs, jusqu’à temps
qu’une solution soit trouvée.

L’utilisation de l’intelligence artificielle dans des buts de tuer des individus est un grave
danger. Tel que mentionné dans le rapport, l’entraînement de l’intelligence se fait à partir
d’un grand jeu de données fiables. Dans un contexte d’une guerre contre le terrorisme, avoir
un jeu de données sur les terroristres s’avère très difficile et peu fiable, car la plupart
d’entre eux s’habille comme des civiles. De plus, cette technologie, une fois tombée dans les mains
des terroristes, pourrait semer terreur au sein d’un pays comme le démontre très bien la vidéo
\sphinxhref{https://www.youtube.com/watch?v=HipTO\_7mUOw\&ab\_channel=FutureofLifeInstitute}{Slaughterbots}
qui promeut l’interdiction de l’usage robot tueur.

En conclusion, l’implémentation de l’intelligence artificielle ainsi que la croissance
exponentielle de celle\sphinxhyphen{}ci va rapporter une tonne de bénéfices tel l’élimination de l’erreur humaine,
l’utilisation de celle\sphinxhyphen{}ci dans des tâches dangereuses, la disponibilité 24/7 de celle\sphinxhyphen{}ci,
la rapidité de prise de décision et les emplois dans ce domaine. De l’autre côté, si l’on veut
que ces bénéfices portent fruit, il faut limiter ou éliminer les impacts négatifs de l’intelligence
artificielle. Cela va se faire graduellement par l’implémentation de règles mondiales sur les jeux
de données utilisés dans l’intelligence machine, l’intelligence machine en temps que tel, l’utilisation
de l’intelligence artificielle dans un environnement de guerre, et ce, dans l’accord de la majorité
des pays en plus de développer et améliorer la cybersécurité. Une idée intéressante pour régler le
problème des jeux de données pourrait être de standardiser les jeux de données qui ne sont pas biaisés
dans le but d’éviter les problèmes. C’est déjà ce que le NIST fait en fournissant des jeux de données
fiables gratuitement.

Finalement, quel est le fonctionnement de l’intelligence artificielle et comment devrait\sphinxhyphen{}elle être utilisée
afin de bénéficier l’être humain? L’hypothèse émise était que si son développement se fait de manière
éthique et s’il est bien encadré, nous pourrions en retirer plus d’avantages que d’inconvénients. Il
était donc important de comprendre le fonctionnement de cette technologie pour pouvoir expliquer et
rationaliser l’utilisation bénéfique de l’intelligence artificielle et d’ainsi le documenter.


\section{Conclusion}
\label{\detokenize{conclusion:conclusion}}\label{\detokenize{conclusion::doc}}
Finalement, quel est le fonctionnement de l’intelligence artificielle et
comment devrait\sphinxhyphen{}elle être utilisée afin de bénéficier l’être humain?
L’hypothèse émise était que si son développement se fait de manière éthique
et s’il est bien encadré, nous pourrions en retirer plus d’avantages que
d’inconvénients. Il était donc important de comprendre le fonctionnement
de cette technologie pour pouvoir expliquer et rationaliser l’utilisation
bénéfique de l’intelligence artificielle et d’ainsi le documenter.

Pour ce faire, il a fallu procéder à l’écriture d’un programme d’OCR, une
forme simple d’intelligence artificielle qui a pour but de reconnaître des
caractères écrits à la main. Grâce à l’information acquise lors de l’écriture
de celui\sphinxhyphen{}ci ainsi que toute la documentation lue pour la préparation du
programme, il a été plus facile de comprendre les aspects d’un réseau neuronal
dans le but de documenter chaque composante du réseau neuronal.

Ce que nous pouvons conclure d’un réseau neuronal après notre documentation,
c’est que cette “intelligence” artificielle, n’est rien d’autre qu’un paquet
de fonctions avec des paramètres qui ont été ajustés par une méthode d’optimisation
dénommée “Back propagation”. Cette méthode est composée de fonctions qui
utilisent des notions de mathématiques telles que les dérivées. Cet algorithme
a lieu lors de chaque “epoch%
\begin{footnote}[2]\sphinxAtStartFootnote
Une “epoch” étant un cycle complet où l’algorithme a traité
le jeu de données qui lui a été fournie une seule fois.
%
\end{footnote}” dans le but d’ajuster l’algorithme d’OCR
graduellement.

Donc, cet algorithme « intelligent ” n’est pas capable de prendre des
décisions elle\sphinxhyphen{}même et doit être surveillé et entraîné par un être humain
à l’aide d’un jeu de données.Cela dans le but de pouvoir répondre à la tâche
précise à la\sphinxhyphen{}quel l’algorithme s’est fait assigner, qui est dans notre
cas la reconnaissance optique des chiffres de 0 à 9. Alors, un algorithme
qui est bon ou mauvais dans sa tâche et donc qui a une bonne ou mauvaise
précision est principalement déterminé par la façon dont l’algorithme a
été entraîné et si le jeu de données utilisé pour l’entraîner est fiable
et diversifié. Dans notre cas, la précision est calculée par le nombre de
prédiction réussite sur le nombre total de prédiction.C’est donc l’entraînement
que provient une partie des biais de l’intelligence artificielle, et donc
un des inconvénients qu’on avait cité dans notre hypothèse.

L’autre inconvénient qui a été mentionné lors de l’hypothèse est l’importante
quantité d’emploi qui risque de disparaître. Or, tel que mentionné dans la
section sur les impacts de l’intelligence artificielle, il n’y a pas seulement
cet inconvénient, et ces autres inconvénients sont accompagnés de plusieurs
avantages intéressants.

Comme il est décrit dans la section des bienfaits et inconvénients, l’usage
de l’apprentissage machine peut amener beaucoup d’avantages ainsi que des
inconvénients. Ces principaux avantages se résument à,
l’élimination de l’erreur humaine, l’utilisation de l’IA dans des tâches
dangereuses, la disponibilité 24/7 de celle\sphinxhyphen{}ci, la rapidité de prise de
décision et les emplois dans ce domaine. Tandis que les inconvénient se
résument au biais de l’IA, diminution de certains types d’emploi, les conflits
mondiaux lié à celle\sphinxhyphen{}ci, l’amélioration des “hacks” et l’utilisation de
celle\sphinxhyphen{}ci dans le but de tuer. Si l’on veut avoir les bénéfices de l’intelligence
artificielle, il faut d’abord s’occuper de diminuer l’impact des inconvénients.
La bonne nouvelle étant que ces inconvénients peuvent être quasi inexistants
si l’IA est implémentée en suivant dans un bon cadre strict ainsi que des
mesures de sécurité. Par exemple, des règles sur l’utilisation des bases
de données et des réglementations sur celles\sphinxhyphen{}ci pourraient diminuer les
biais dans les programmes d’apprentissage machine. Si ces conditions sont
respectées, il est préférable de penser que l’usage de l’intelligence
artificielle pourrait faire évoluer la société.

Pour répondre à la thèse, l’intelligence artificielle fonctionne par l’étude
de grosses bases de données par un programme. Celui\sphinxhyphen{}ci va ensuite faire
varier ses paramètres clés dans le but d’ajuster certaines fonctions. Ces
fonctions permettent de trouver des tendances dans le jeu de données et de
les utiliser dans le but de répondre à la tâche attribuée. De plus,
l’intelligence artificielle peut constituer un bénéfice pour l’être humain
à condition que celle\sphinxhyphen{}ci soit bien encadrée.

Il ne faut toutefois pas écarter le fait que le futur n’est jamais certain,
et qu’on ne peut prédire à 100\% ce que l’intelligence artificielle va
ressembler dans 50 ans. Mais, avec les avancées technologiques des dernières
années qui ne semblent pas s’arrêter, il serait juste de croire que l’IA
a un futur prometteur. Avec ces avancés prometteuses, ce pourrait\sphinxhyphen{}il qu’il
y ait une sorte d’intelligence au\sphinxhyphen{}delà de celle humaine et artificielle dans le futur ?


\bigskip\hrule\bigskip



\section{Bibliographie}
\label{\detokenize{zbib:bibliographie}}\label{\detokenize{zbib::doc}}


\begin{sphinxthebibliography}{Les cont}
\bibitem[Anonymous, n.d.a]{zbib:noauthor-impact-nodate}
None to claim their bones (n.d.). \sphinxstyleemphasis{Impact of inverting grayscale values on mnist dataset}.
\bibitem[Anonymous, n.d.b]{zbib:noauthor-why-nodate}
None to claim their bones (n.d.). \sphinxstyleemphasis{why we should use gray scale for image processing}.
\bibitem[Anonymous, n.d.c]{zbib:noauthor-blas-nodate}
None to claim their bones (n.d.). \sphinxstyleemphasis{BLAS (Basic Linear Algebra Subprograms)}.
\bibitem[Anonymous, 2020]{zbib:noauthor-supervised-2020}
None to claim their bones (2020 , October). \sphinxstyleemphasis{Supervised learning}. Page Version ID: 982281641.
\bibitem[InnovationQuebec, 2018]{zbib:gouvqc}
et Innovation Québec, É. (2018). \sphinxstyleemphasis{Les avantages et inconvénients de l’intelligence artificielle}.
\bibitem[futura\sphinxhyphen{}sciences, n.d.]{zbib:futura-sciences}
futura\sphinxhyphen{}sciences (n.d.). \sphinxstyleemphasis{Potentiel d’action}.
\bibitem[Goodfellow et al., 2016]{zbib:goodfellow-et-al-2016}
Goodfellow, I., Bengio, Y., \& Courville, A. (2016). \sphinxstyleemphasis{Deep Learning}. MIT Press. http://www.deeplearningbook.org.
\bibitem[Grother et al., 2019]{zbib:nistbias}
Grother, P., Ngan, M., \& Hanaoka, K. (2019). \sphinxstyleemphasis{Face Recognition Vendor Test (FRVT) Part 3: Demographic Effects}. NIST.
\bibitem[LAURO, n.d.]{zbib:ubiquity}
LAURO, D. M. (n.d.). \sphinxstyleemphasis{HUMAN BRAIN AND NEURAL NETWORK BEHAVIOR A COMPARISON}.
\bibitem[Metz, 2020]{zbib:cnnportland}
Metz, R. (2020). \sphinxstyleemphasis{Portland passes broadest facial recognition ban in the US}.
\bibitem[Nielsen, 2019]{zbib:michael}
Nielsen, M. A. (2019). \sphinxstyleemphasis{Neural Networl and Deep Learning}.
\bibitem[Popper, 2016]{zbib:nytimes}
Popper, N. (2016). \sphinxstyleemphasis{The Robots Are Coming for Wall Street}.
\bibitem[RFisher \& Wolfart, 2003]{zbib:hipr2}
R. Fisher, S. Perkins, A. W., \& Wolfart, E. (2003). \sphinxstyleemphasis{Pixel Values}.
\bibitem[Les contributeurs de Wikipedia, 2020]{zbib:wikitf}
Les contributeurs de Wikipedia (2020). \sphinxstyleemphasis{TensorFlow}.
\bibitem[The Numpy documentation team, 2020]{zbib:numpy}
The Numpy documentation team (2020). \sphinxstyleemphasis{About us}.
\bibitem[The TensorFlow developper team, 2015]{zbib:tfpaper}
The TensorFlow developper team (2015). \sphinxstyleemphasis{TensorFlow: Large\sphinxhyphen{}Scale Machine Learning on Heterogeneous Distributed Systems}.
\bibitem[The TensorFlow developper team, 2020]{zbib:tfmain}
The TensorFlow developper team (2020). \sphinxstyleemphasis{TensorFlow}.
\end{sphinxthebibliography}







\renewcommand{\indexname}{Index}
\printindex
\end{document}